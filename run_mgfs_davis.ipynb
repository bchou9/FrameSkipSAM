{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc92286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 validation sequences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SAM2VideoPredictor(\n",
       "  (image_encoder): ImageEncoder(\n",
       "    (trunk): Hiera(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 144, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-1): 2 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=144, out_features=432, bias=True)\n",
       "            (proj): Linear(in_features=144, out_features=144, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=144, out_features=576, bias=True)\n",
       "              (1): Linear(in_features=576, out_features=144, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (2): MultiScaleBlock(\n",
       "          (norm1): LayerNorm((144,), eps=1e-06, elementwise_affine=True)\n",
       "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (qkv): Linear(in_features=144, out_features=864, bias=True)\n",
       "            (proj): Linear(in_features=288, out_features=288, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=288, out_features=1152, bias=True)\n",
       "              (1): Linear(in_features=1152, out_features=288, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (proj): Linear(in_features=144, out_features=288, bias=True)\n",
       "        )\n",
       "        (3-7): 5 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=288, out_features=864, bias=True)\n",
       "            (proj): Linear(in_features=288, out_features=288, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=288, out_features=1152, bias=True)\n",
       "              (1): Linear(in_features=1152, out_features=288, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (8): MultiScaleBlock(\n",
       "          (norm1): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (qkv): Linear(in_features=288, out_features=1728, bias=True)\n",
       "            (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=576, out_features=2304, bias=True)\n",
       "              (1): Linear(in_features=2304, out_features=576, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (proj): Linear(in_features=288, out_features=576, bias=True)\n",
       "        )\n",
       "        (9-43): 35 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=576, out_features=1728, bias=True)\n",
       "            (proj): Linear(in_features=576, out_features=576, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=576, out_features=2304, bias=True)\n",
       "              (1): Linear(in_features=2304, out_features=576, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "        (44): MultiScaleBlock(\n",
       "          (norm1): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "          (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "            (qkv): Linear(in_features=576, out_features=3456, bias=True)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=1152, out_features=4608, bias=True)\n",
       "              (1): Linear(in_features=4608, out_features=1152, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "          (proj): Linear(in_features=576, out_features=1152, bias=True)\n",
       "        )\n",
       "        (45-47): 3 x MultiScaleBlock(\n",
       "          (norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiScaleAttention(\n",
       "            (qkv): Linear(in_features=1152, out_features=3456, bias=True)\n",
       "            (proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=1152, out_features=4608, bias=True)\n",
       "              (1): Linear(in_features=4608, out_features=1152, bias=True)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): FpnNeck(\n",
       "      (position_encoding): PositionEmbeddingSine()\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (conv): Conv2d(1152, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (conv): Conv2d(576, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Sequential(\n",
       "          (conv): Conv2d(288, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Sequential(\n",
       "          (conv): Conv2d(144, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mask_downsample): Conv2d(1, 1, kernel_size=(4, 4), stride=(4, 4))\n",
       "  (memory_attention): MemoryAttention(\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x MemoryAttentionLayer(\n",
       "        (self_attn): RoPEAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (cross_attn_image): RoPEAttention(\n",
       "          (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (k_proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (v_proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        (dropout3): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (memory_encoder): MemoryEncoder(\n",
       "    (mask_downsampler): MaskDownSampler(\n",
       "      (encoder): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (7): LayerNorm2d()\n",
       "        (8): GELU(approximate='none')\n",
       "        (9): Conv2d(64, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (10): LayerNorm2d()\n",
       "        (11): GELU(approximate='none')\n",
       "        (12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (pix_feat_proj): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (fuser): Fuser(\n",
       "      (proj): Identity()\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x CXBlock(\n",
       "          (dwconv): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=256)\n",
       "          (norm): LayerNorm2d()\n",
       "          (pwconv1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "          (pwconv2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (drop_path): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (position_encoding): PositionEmbeddingSine()\n",
       "    (out_proj): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (sam_prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (sam_mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLP(\n",
       "            (layers): ModuleList(\n",
       "              (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (1): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            )\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (obj_score_token): Embedding(1, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (conv_s0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv_s1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "        (act): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "    (pred_obj_score_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=1, bias=True)\n",
       "      )\n",
       "      (act): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (obj_ptr_proj): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-2): 3 x Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (act): ReLU()\n",
       "  )\n",
       "  (obj_ptr_tpos_proj): Linear(in_features=256, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os, json, time, subprocess, pathlib\n",
    "from pathlib import Path\n",
    "from davis2017.davis import DAVIS\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# ── USER‐CONFIGURABLE PATHS ──────────────────────────────────────────────────\n",
    "DAVIS_ROOT  = Path(\"./data/davis/DAVIS\")          # ← point this at your DAVIS folder\n",
    "OUT_DIR     = Path(\"./data/sam2_preds_MA_0.15\")           # ← where we’ll write out PNGs\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ── STEP 1: load the DAVIS “val” split (semi‐supervised task) ───────────────────\n",
    "ds = DAVIS(str(DAVIS_ROOT), task=\"semi-supervised\", subset=\"val\", resolution=\"480p\")\n",
    "print(f\"Loaded {len(ds.sequences)} validation sequences\")\n",
    "\n",
    "# ── STEP 2: build SAM 2 video predictor ─────────────────────────────────────────\n",
    "from sam2.build_sam import build_sam2_video_predictor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "sam2_checkpoint = \"./checkpoints/sam2.1_hiera_large.pt\"   # ← adjust if needed\n",
    "model_cfg       = \"configs/sam2.1/sam2.1_hiera_l.yaml\"   # ← adjust if needed\n",
    "\n",
    "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)\n",
    "predictor.to(device)   # make sure model is on CUDA if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd669c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing sequence: bike-packing ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 69/69 [00:01<00:00, 49.72it/s]\n",
      "/home/wei/FrameSkipSAM/sam2/sam2_video_predictor.py:952: UserWarning: cannot import name '_C' from 'sam2' (/home/wei/FrameSkipSAM/sam2/__init__.py)\n",
      "\n",
      "Skipping the post-processing step due to the error above. You can still use SAM 2 and it's OK to ignore the error above, although some post-processing functionality may be limited (which doesn't affect the results in most cases; see https://github.com/facebookresearch/sam2/blob/main/INSTALL.md).\n",
      "  pred_masks_gpu = fill_holes_in_mask_scores(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 unique non‐black colors in bike-packing/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 69/69 [00:11<00:00,  5.95it/s]7it/s]\n",
      "Propagating bike-packing: 100%|██████████| 69/69 [00:11<00:00,  5.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 43 frames due to low MAD.\n",
      "→ Processed 69 frames in bike-packing at 6.56 FPS\n",
      "→ Saved all predicted masks for bike-packing in data/sam2_preds_MA_0.15/bike-packing\n",
      "\n",
      "=== Processing sequence: blackswan ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 50/50 [00:00<00:00, 57.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique non‐black colors in blackswan/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 50/50 [00:03<00:00, 13.72it/s]/s]\n",
      "Propagating blackswan: 100%|██████████| 50/50 [00:03<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 47 frames due to low MAD.\n",
      "→ Processed 50 frames in blackswan at 15.72 FPS\n",
      "→ Saved all predicted masks for blackswan in data/sam2_preds_MA_0.15/blackswan\n",
      "\n",
      "=== Processing sequence: bmx-trees ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 80/80 [00:01<00:00, 57.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 unique non‐black colors in bmx-trees/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 80/80 [00:19<00:00,  4.13it/s]/s]\n",
      "Propagating bmx-trees: 100%|██████████| 80/80 [00:19<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 14 frames due to low MAD.\n",
      "→ Processed 80 frames in bmx-trees at 4.32 FPS\n",
      "→ Saved all predicted masks for bmx-trees in data/sam2_preds_MA_0.15/bmx-trees\n",
      "\n",
      "=== Processing sequence: breakdance ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 84/84 [00:01<00:00, 56.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique non‐black colors in breakdance/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 84/84 [00:16<00:00,  5.22it/s]t/s]\n",
      "Propagating breakdance: 100%|██████████| 84/84 [00:16<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 11 frames due to low MAD.\n",
      "→ Processed 84 frames in breakdance at 5.50 FPS\n",
      "→ Saved all predicted masks for breakdance in data/sam2_preds_MA_0.15/breakdance\n",
      "\n",
      "=== Processing sequence: camel ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 90/90 [00:01<00:00, 60.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique non‐black colors in camel/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 90/90 [00:07<00:00, 12.26it/s]\n",
      "Propagating camel: 100%|██████████| 90/90 [00:07<00:00, 12.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 79 frames due to low MAD.\n",
      "→ Processed 90 frames in camel at 13.94 FPS\n",
      "→ Saved all predicted masks for camel in data/sam2_preds_MA_0.15/camel\n",
      "\n",
      "=== Processing sequence: car-roundabout ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 75/75 [00:01<00:00, 60.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique non‐black colors in car-roundabout/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 75/75 [00:06<00:00, 11.91it/s].37it/s]\n",
      "Propagating car-roundabout: 100%|██████████| 75/75 [00:06<00:00, 11.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 66 frames due to low MAD.\n",
      "→ Processed 75 frames in car-roundabout at 13.28 FPS\n",
      "→ Saved all predicted masks for car-roundabout in data/sam2_preds_MA_0.15/car-roundabout\n",
      "\n",
      "=== Processing sequence: car-shadow ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 40/40 [00:00<00:00, 57.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique non‐black colors in car-shadow/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 40/40 [00:05<00:00,  7.26it/s]t/s]\n",
      "Propagating car-shadow: 100%|██████████| 40/40 [00:05<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 22 frames due to low MAD.\n",
      "→ Processed 40 frames in car-shadow at 7.70 FPS\n",
      "→ Saved all predicted masks for car-shadow in data/sam2_preds_MA_0.15/car-shadow\n",
      "\n",
      "=== Processing sequence: cows ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 104/104 [00:01<00:00, 60.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique non‐black colors in cows/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 104/104 [00:07<00:00, 13.13it/s]\n",
      "Propagating cows: 100%|██████████| 104/104 [00:07<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 96 frames due to low MAD.\n",
      "→ Processed 104 frames in cows at 14.93 FPS\n",
      "→ Saved all predicted masks for cows in data/sam2_preds_MA_0.15/cows\n",
      "\n",
      "=== Processing sequence: dance-twirl ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 90/90 [00:01<00:00, 60.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique non‐black colors in dance-twirl/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Propagating dance-twirl:  20%|██        | 18/90 [00:02<00:12,  5.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 7 frames due to low MAD.\n",
      "→ Processed 50 frames in kite-surf at 3.11 FPS\n",
      "→ Saved all predicted masks for kite-surf in data/sam2_preds_MA_0.15/kite-surf\n",
      "\n",
      "=== Processing sequence: lab-coat ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 47/47 [00:00<00:00, 54.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 unique non‐black colors in lab-coat/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 47/47 [00:07<00:00,  5.97it/s]s]\n",
      "Propagating lab-coat: 100%|██████████| 47/47 [00:07<00:00,  5.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 35 frames due to low MAD.\n",
      "→ Processed 47 frames in lab-coat at 6.61 FPS\n",
      "→ Saved all predicted masks for lab-coat in data/sam2_preds_MA_0.15/lab-coat\n",
      "\n",
      "=== Processing sequence: libby ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 49/49 [00:00<00:00, 58.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 unique non‐black colors in libby/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 49/49 [00:05<00:00,  9.00it/s]\n",
      "Propagating libby: 100%|██████████| 49/49 [00:05<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 33 frames due to low MAD.\n",
      "→ Processed 49 frames in libby at 9.71 FPS\n",
      "→ Saved all predicted masks for libby in data/sam2_preds_MA_0.15/libby\n",
      "\n",
      "=== Processing sequence: loading ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 50/50 [00:00<00:00, 60.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 unique non‐black colors in loading/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 50/50 [00:05<00:00,  9.10it/s]]\n",
      "Propagating loading: 100%|██████████| 50/50 [00:05<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped 43 frames due to low MAD.\n",
      "→ Processed 50 frames in loading at 10.56 FPS\n",
      "→ Saved all predicted masks for loading in data/sam2_preds_MA_0.15/loading\n",
      "\n",
      "=== Processing sequence: mbike-trick ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 79/79 [00:01<00:00, 60.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 unique non‐black colors in mbike-trick/00000.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Propagating mbike-trick:  72%|███████▏  | 57/79 [00:08<00:03,  6.17it/s]"
     ]
    }
   ],
   "source": [
    "# ── STEP 3: for each DAVIS sequence, run SAM 2 and save outputs ───────────────\n",
    "fps = 0.0\n",
    "for seq in ds.sequences:\n",
    "    print(f\"\\n=== Processing sequence: {seq} ===\")\n",
    "\n",
    "    # 3a) directories of images & GT masks for this sequence\n",
    "    img_dir  = DAVIS_ROOT / \"JPEGImages\"  / \"480p\" / seq\n",
    "    mask_dir = DAVIS_ROOT / \"Annotations\" / \"480p\" / seq\n",
    "\n",
    "    img_paths  = sorted(img_dir.glob(\"*.jpg\"))\n",
    "    mask_paths = sorted(mask_dir.glob(\"*.png\"))\n",
    "    assert len(img_paths) == len(mask_paths), (\n",
    "        f\"Image/mask count mismatch in {seq}: {len(img_paths)} vs {len(mask_paths)}\"\n",
    "    )\n",
    "\n",
    "    # 3b) initialize inference state by “loading” this sequence as a video\n",
    "    #     We pass the *directory* of frames to init_state.  Internally, it will call\n",
    "    #     `load_video_frames(video_path=video_dir, ...)` and store all frames in memory.\n",
    "    video_dir = str(img_dir)  # e.g. \"./data/davis/DAVIS/JPEGImages/480p/<seq>\"\n",
    "    inference_state = predictor.init_state(\n",
    "        video_path=video_dir,\n",
    "        offload_video_to_cpu=False,\n",
    "        offload_state_to_cpu=False,\n",
    "        async_loading_frames=False\n",
    "    )\n",
    "    \n",
    "\n",
    "# (b) load the single “00000.png” which contains two different colored regions\n",
    "    rgb = iio.imread(str(mask_dir / \"00000.png\"))  # shape (H, W, 3)\n",
    "    H, W, C = rgb.shape\n",
    "    assert C == 3, \"Expected a 3‐channel (RGB) first‐frame mask.\"\n",
    "\n",
    "    # (c) find all unique RGB colors except black\n",
    "    flat = rgb.reshape(-1, 3)                        # shape (H*W, 3)\n",
    "    uniq_colors = np.unique(flat, axis=0)            # shape (K, 3), where K ≤ (H*W)\n",
    "    # Remove the black color (0,0,0) if present\n",
    "    non_black = [tuple(c) for c in uniq_colors if not np.all(c == 0)]\n",
    "    if len(non_black) == 0:\n",
    "        raise RuntimeError(f\"No non‐black colors found in {seq}/00000.png\")\n",
    "\n",
    "    # (d) for each unique non‐black color, build a 2D boolean mask and register it\n",
    "    print(f\"Found {len(non_black)} unique non‐black colors in {seq}/00000.png\")\n",
    "    for idx, color in enumerate(non_black):\n",
    "        # color is something like (200, 0, 0) or (0, 200, 0)\n",
    "        R, G, B = color\n",
    "        # build a binary mask: True where pixel == this color\n",
    "        bin_mask = np.logical_and.reduce([\n",
    "            rgb[:, :, 0] == R,\n",
    "            rgb[:, :, 1] == G,\n",
    "            rgb[:, :, 2] == B\n",
    "        ])  # shape (H, W), dtype=bool\n",
    "\n",
    "        # wrap as torch.bool on the same device as SAM 2\n",
    "        mask2d = torch.from_numpy(bin_mask).to(device)\n",
    "\n",
    "        # register this mask as object `idx`\n",
    "        predictor.add_new_mask(\n",
    "            inference_state=inference_state,\n",
    "            frame_idx=0,\n",
    "            obj_id=idx,  # choose 0,1,2,… per color\n",
    "            mask=mask2d\n",
    "        )\n",
    "\n",
    "    # 3e) now propagate through all frames.  As each new frame is processed,\n",
    "    #     propagate_in_video yields (frame_idx, [obj_ids], video_res_masks).\n",
    "    #\n",
    "    #     We’ll save each mask as “00000.png”, “00001.png”, … under OUT_DIR/<seq>/\n",
    "    seq_out_dir = OUT_DIR / seq\n",
    "    seq_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_time = 0.0\n",
    "    for frame_idx, obj_ids, video_res_masks in tqdm(\n",
    "        predictor.propagate_in_video(inference_state),\n",
    "        total=len(img_paths),\n",
    "        desc=f\"Propagating {seq}\"\n",
    "    ):\n",
    "        total_time += time.time() - t0\n",
    "        # # ‣ frame_idx is an integer (1,2,3,…).  video_res_masks is a tensor of shape\n",
    "        # #   (num_objects, H, W).  For DAVIS, num_objects==1.\n",
    "        # #\n",
    "        # # ‣ Thresholding has already happened internally; `video_res_masks` is\n",
    "        # #   a float‐tensor where positive values correspond to predicted “object.”\n",
    "        # mask_np = (video_res_masks[0].cpu().numpy() > 0.0).astype(np.uint8) * 255\n",
    "\n",
    "        # # Save with zero‐padded five digits to match DAVIS naming:\n",
    "        # save_name = f\"{frame_idx:05d}.png\"\n",
    "        # save_path = seq_out_dir / save_name\n",
    "        # iio.imwrite(str(save_path), mask_np)\n",
    "\n",
    "        # Suppose `video_res_masks` is whatever you get from propagate_in_video:\n",
    "        #   • If there is only one object, it may be a 2D tensor of shape (H, W)\n",
    "        #   • If there are multiple objects, it will be a 3D tensor of shape (O, H, W)\n",
    "\n",
    "        pred_np = video_res_masks.cpu().numpy()   # dtype=float32 or float; # ───────────────────────────────────────────────────────────────\n",
    "        # Assume you already did:\n",
    "        #   pred_np = video_res_masks.cpu().numpy()\n",
    "\n",
    "        # 1) Check how many dimensions `pred_np` has:\n",
    "        if pred_np.ndim == 2:\n",
    "            # Case A: single object, shape = (H, W)\n",
    "            H, W = pred_np.shape\n",
    "            O = 1\n",
    "            pred_np = pred_np[np.newaxis, ...]  # -> now shape (1, H, W)\n",
    "\n",
    "        elif pred_np.ndim == 3:\n",
    "            # Could be either:\n",
    "            #  (A) shape = (1, H, W)   ← single object with a leading axis\n",
    "            #  (B) shape = (O, H, W)   ← multiple objects, no extra channel axis\n",
    "            if pred_np.shape[0] == 1:\n",
    "                # Treat as “one‐object” → squeeze to (1, H, W) (already fits our convention)\n",
    "                O, H, W = pred_np.shape\n",
    "            else:\n",
    "                # Multi‐object already: (O, H, W)\n",
    "                O, H, W = pred_np.shape\n",
    "            # (no need to reshape because it’s already (O, H, W))\n",
    "\n",
    "        elif pred_np.ndim == 4:\n",
    "            # Some SAM 2 builds return (O, 1, H, W). In that case:\n",
    "            #   • pred_np.shape = (O, 1, H, W)\n",
    "            #   → we want to drop the “channel” dimension (axis=1).\n",
    "            O = pred_np.shape[0]\n",
    "            H = pred_np.shape[2]\n",
    "            W = pred_np.shape[3]\n",
    "            pred_np = pred_np[:, 0, :, :]  # now shape (O, H, W)\n",
    "\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unexpected mask array with ndim={pred_np.ndim}, shape={pred_np.shape}\")\n",
    "\n",
    "        # At this point:\n",
    "        #   • pred_np is guaranteed to have shape (O, H, W)\n",
    "        #   • O, H, W are set correctly\n",
    "        # ───────────────────────────────────────────────────────────────\n",
    "\n",
    "        # Now you can build your colored output exactly as before:\n",
    "\n",
    "        colored = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "\n",
    "        for i in range(O):\n",
    "            mask_i = (pred_np[i] > 0.0)   # boolean mask (H, W)\n",
    "            if not mask_i.any():\n",
    "                continue\n",
    "            R, G, B = non_black[i]  # the original RGB for object i\n",
    "            colored[mask_i, 0] = R\n",
    "            colored[mask_i, 1] = G\n",
    "            colored[mask_i, 2] = B\n",
    "\n",
    "        save_name = f\"{frame_idx:05d}.png\"\n",
    "        save_path = seq_out_dir / save_name\n",
    "        iio.imwrite(str(save_path), colored)\n",
    "        t0 = time.time()\n",
    "\n",
    "    # record FPS\n",
    "    cur_fps = len(img_paths) / total_time if total_time > 0 else 0.0\n",
    "    fps += cur_fps\n",
    "    print(f\"→ Processed {len(img_paths)} frames in {seq} at {cur_fps:.2f} FPS\")\n",
    "\n",
    "    print(f\"→ Saved all predicted masks for {seq} in {seq_out_dir}\")\n",
    "\n",
    "# save FPS into a file\n",
    "fps_file = OUT_DIR / \"fps.txt\"\n",
    "with open(fps_file, \"w\") as f:\n",
    "    f.write(f\"{fps / len(ds.sequences):.2f}\\n\")\n",
    "\n",
    "print(\"\\nAll sequences processed.\")\n",
    "print(f\"Average FPS across all sequences: {fps / len(ds.sequences):.2f}\")\n",
    "print(f\"Your SAM 2 masks live under: {OUT_DIR}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
